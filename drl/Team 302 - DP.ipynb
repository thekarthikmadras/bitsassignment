{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e706b20",
   "metadata": {},
   "source": [
    "### Group ID: 302\n",
    "### Group Members Name with Student ID:\n",
    "1. KARTHIKEYAN J - 2024AA05372\n",
    "2. JANGALE SAVEDANA SUBHASH PRATIBHA - 2024AA05187\n",
    "3. GANAPATHY SUBRAMANIAN S - 2024AA05188\n",
    "4. ANANDAN A - 2024AA05269"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e826906",
   "metadata": {},
   "source": [
    "### The Smart Supplier: Optimizing Orders in a Fluctuating Market - 6 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beffcce",
   "metadata": {},
   "source": [
    "Develop a reinforcement learning agent using dynamic programming to help a Smart Supplier decide which products to manufacture and sell each day to maximize profit. The agent must learn the optimal policy for choosing daily production quantities, considering its limited raw materials and the unpredictable daily demand and selling prices for different products.\n",
    "\n",
    "#### **Scenario**\n",
    " A small Smart Supplier manufactures two simple products: Product A and Product B. Each day, the supplier has a limited amount of raw material. The challenge is that the market demand and selling price for Product A and Product B change randomly each day, making some products more profitable than others at different times. The supplier needs to decide how much of each product to produce to maximize profit while managing their limited raw material.\n",
    "\n",
    "#### **Objective**\n",
    "The Smart Supplier's agent must learn the optimal policy π∗ using dynamic programming (Value Iteration or Policy Iteration) to decide how many units of Product A and Product B to produce each day to maximize the total profit over the fixed number of days, given the daily changing market conditions and limited raw material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4feac",
   "metadata": {},
   "source": [
    "### --- 1. Custom Environment Creation (SmartSupplierEnv) --- ( 1 Mark )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a4a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define market states and their product prices\n",
    "# Structure: {Market_State_ID: {'A_price': X, 'B_price': Y}}\n",
    "# Define product raw material costs\n",
    "# Define actions: (num_A, num_B, raw_material_cost_precalculated)\n",
    "        # Action ID mapping:\n",
    "        # 0: Produce_2A_0B\n",
    "        # 1: Produce_1A_2B\n",
    "        # 2: Produce_0A_5B\n",
    "        # 3: Produce_3A_0B\n",
    "        # 4: Do_Nothing\n",
    "\n",
    " # Define state space dimensions\n",
    "        # Current Day: 1 to num_days\n",
    "        # Current Raw Material: 0 to initial_raw_material\n",
    "        # Current Market State: 1 or 2\n",
    "\n",
    "# get reward function\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class SmartSupplierEnv:\n",
    "    \"\"\"\n",
    "    Custom environment for the Smart Supplier problem.\n",
    "    Handles state transitions, actions, market states, and reward calculation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_days=5, initial_rm=10):\n",
    "        self.num_days = num_days\n",
    "        self.initial_rm = initial_rm\n",
    "\n",
    "        # Define action space with associated costs\n",
    "        self.actions = {\n",
    "            0: (2, 0, 4),  # Produce 2A, 0B → Costs 4 RM\n",
    "            1: (1, 2, 4),  # Produce 1A, 2B → Costs 4 RM\n",
    "            2: (0, 5, 5),  # Produce 0A, 5B → Costs 5 RM\n",
    "            3: (3, 0, 6),  # Produce 3A, 0B → Costs 6 RM\n",
    "            4: (0, 0, 0),  # Do Nothing → Costs 0 RM\n",
    "        }\n",
    "\n",
    "        # Define market states with associated prices\n",
    "        self.market_states = {\n",
    "            1: {'A_price': 8, 'B_price': 2},  # High demand for A\n",
    "            2: {'A_price': 3, 'B_price': 5},  # High demand for B\n",
    "        }\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\"\n",
    "        Generate all valid states in the environment.\n",
    "        Each state is a tuple: (current_day, remaining_raw_material, market_state)\n",
    "        \"\"\"\n",
    "        return [(day, rm, mkt) \n",
    "                for day in range(1, self.num_days + 1)\n",
    "                for rm in range(0, self.initial_rm + 1)\n",
    "                for mkt in [1, 2]]\n",
    "\n",
    "    def get_possible_actions(self, rm_left):\n",
    "        \"\"\"\n",
    "        Return list of action IDs possible given current raw material.\n",
    "        \"\"\"\n",
    "        return [a for a, (_, _, cost) in self.actions.items() if cost <= rm_left]\n",
    "\n",
    "    def get_reward(self, action_id, market_state):\n",
    "        \"\"\"\n",
    "        Calculate profit earned from a given action in a specific market state.\n",
    "        \"\"\"\n",
    "        a_units, b_units, _ = self.actions[action_id]\n",
    "        prices = self.market_states[market_state]\n",
    "        return a_units * prices['A_price'] + b_units * prices['B_price']\n",
    "\n",
    "    def transition(self, state, action_id):\n",
    "        \"\"\"\n",
    "        Perform transition from one state to next state based on action.\n",
    "        Returns (next_state, reward). If terminal, next_state is None.\n",
    "        \"\"\"\n",
    "        day, rm, market = state\n",
    "        _, _, cost = self.actions[action_id]\n",
    "\n",
    "        if cost > rm:\n",
    "            return state, 0  # Invalid move: return same state, zero reward\n",
    "\n",
    "        reward = self.get_reward(action_id, market)\n",
    "\n",
    "        # Move to next day, reset raw material\n",
    "        next_day = day + 1\n",
    "        if next_day > self.num_days:\n",
    "            return None, reward  # End of episode\n",
    "\n",
    "        next_market = random.choice([1, 2])\n",
    "        return (next_day, self.initial_rm, next_market), reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b2487",
   "metadata": {},
   "source": [
    "### --- 2. Dynamic Programming Implementation (Value Iteration or Policy Iteration) --- (2 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "027db857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration function\n",
    "\n",
    "def value_iteration(env, gamma=1.0, theta=1e-4):\n",
    "    \"\"\"\n",
    "    Performs Value Iteration to compute the optimal policy.\n",
    "    Returns:\n",
    "        V: State-value function (dict)\n",
    "        policy: Optimal action for each state (dict)\n",
    "    \"\"\"\n",
    "    V = {state: 0 for state in env.get_all_states()}\n",
    "    policy = {}\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = V.copy()\n",
    "\n",
    "        for state in env.get_all_states():\n",
    "            day, rm, mkt = state\n",
    "            if day > env.num_days:\n",
    "                continue  # Terminal state\n",
    "\n",
    "            best_action_value = float('-inf')\n",
    "            best_action = None\n",
    "\n",
    "            for action in env.get_possible_actions(rm):\n",
    "                total_value = 0\n",
    "                reward = env.get_reward(action, mkt)\n",
    "\n",
    "                # For value iteration, consider future market states\n",
    "                for next_market in [1, 2]:\n",
    "                    next_day = day + 1\n",
    "                    if next_day > env.num_days:\n",
    "                        future_val = 0\n",
    "                    else:\n",
    "                        next_state = (next_day, env.initial_rm, next_market)\n",
    "                        future_val = V[next_state]\n",
    "                    prob = 0.5  # Market transitions are equally likely\n",
    "                    total_value += prob * (reward + gamma * future_val)\n",
    "\n",
    "                if total_value > best_action_value:\n",
    "                    best_action_value = total_value\n",
    "                    best_action = action\n",
    "\n",
    "            new_V[state] = best_action_value\n",
    "            policy[state] = best_action\n",
    "            delta = max(delta, abs(V[state] - best_action_value))\n",
    "\n",
    "        V = new_V\n",
    "        iteration += 1\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    print(f\"Value iteration converged in {iteration} iterations.\")\n",
    "    return V, policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cfa71",
   "metadata": {},
   "source": [
    "#### --- 3. Simulation and Policy Analysis ---  ( 1 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62490896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate policy function - Simulates the learned policy over multiple runs to evaluate performance\n",
    "\n",
    "def simulate_policy(env, policy, runs=1000):\n",
    "    \"\"\"\n",
    "    Simulate the policy over multiple runs to estimate average total reward.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "\n",
    "    for _ in range(runs):\n",
    "        state = (1, env.initial_rm, random.choice([1, 2]))\n",
    "        total_reward = 0\n",
    "\n",
    "        while state is not None:\n",
    "            action = policy.get(state, 4)  # Default to 'Do Nothing' if state not found\n",
    "            next_state, reward = env.transition(state, action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    average_reward = np.mean(total_rewards)\n",
    "    print(f\"Average total reward over {runs} runs: ${average_reward:.2f}\")\n",
    "    return average_reward\n",
    "\n",
    "# analyze policy function - Analyzes and prints snippets of the learned optimal policy\n",
    "\n",
    "def analyze_policy(policy, env):\n",
    "    \"\"\"\n",
    "    Analyzes how the learned policy behaves across different conditions.\n",
    "    \"\"\"\n",
    "    print(\"Policy Analysis for Day 1 with 10 RM:\")\n",
    "    for market in [1, 2]:\n",
    "        state = (1, 10, market)\n",
    "        action = policy.get(state)\n",
    "        a_units, b_units, _ = env.actions[action]\n",
    "        print(f\"  Market State {market}: Action → {a_units}A and {b_units}B\")\n",
    "\n",
    "    print(\"\\nPolicy behavior with low RM:\")\n",
    "    for rm in range(0, 6):\n",
    "        state = (1, rm, 1)\n",
    "        action = policy.get(state, None)\n",
    "        if action is not None:\n",
    "            a_units, b_units, _ = env.actions[action]\n",
    "            print(f\"  RM = {rm}: Action → {a_units}A and {b_units}B\")\n",
    "\n",
    "    print(\"\\nPolicy on last day (Day 5):\")\n",
    "    for market in [1, 2]:\n",
    "        state = (5, 10, market)\n",
    "        action = policy.get(state)\n",
    "        a_units, b_units, _ = env.actions[action]\n",
    "        print(f\"  Market {market}: Action → {a_units}A and {b_units}B\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305647e",
   "metadata": {},
   "source": [
    "#### --- 4. Impact of Dynamics Analysis --- (1 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2448380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discusses the impact of dynamic market prices on the optimal policy.\n",
    "# This section should primarily be a written explanation in the report.\n",
    "\n",
    "# In the markdown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af5feb3",
   "metadata": {},
   "source": [
    "# Dynamic Market Impact Discussion (To include in your markdown/report):\n",
    "\n",
    "If the market state were fixed (e.g., always Market State 1), the optimal policy would favor producing Product A heavily due to its consistently higher reward.\n",
    "\n",
    "However, because the market changes randomly each day, the policy learned using dynamic programming becomes adaptive. It accounts for both possibilities and spreads risk—choosing more flexible or balanced production when uncertainty is high.\n",
    "\n",
    "On the last day, the policy tends to be more aggressive, using up all available resources since there's no future to plan for. This is a clear indicator of the dynamic programming approach considering the finite time horizon in decision-making.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e6570fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged in 6 iterations.\n",
      "Average total reward over 1000 runs: $122.49\n",
      "Policy Analysis for Day 1 with 10 RM:\n",
      "  Market State 1: Action → 3A and 0B\n",
      "  Market State 2: Action → 0A and 5B\n",
      "\n",
      "Policy behavior with low RM:\n",
      "  RM = 0: Action → 0A and 0B\n",
      "  RM = 1: Action → 0A and 0B\n",
      "  RM = 2: Action → 0A and 0B\n",
      "  RM = 3: Action → 0A and 0B\n",
      "  RM = 4: Action → 2A and 0B\n",
      "  RM = 5: Action → 2A and 0B\n",
      "\n",
      "Policy on last day (Day 5):\n",
      "  Market 1: Action → 3A and 0B\n",
      "  Market 2: Action → 0A and 5B\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "env = SmartSupplierEnv()\n",
    "V, policy = value_iteration(env)\n",
    "simulate_policy(env, policy)\n",
    "analyze_policy(policy, env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a657ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
