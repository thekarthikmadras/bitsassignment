{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f8abe90-cabe-44de-a7c7-c8eee462936f",
   "metadata": {},
   "source": [
    "<h2>Information Retrieval Assignment 1</h2>\n",
    "<h4>Group ID: 26</h4>\n",
    "<h4>Group Members Name with Student ID:</h4>\n",
    "<h4>1. KARTHIKEYAN J - 2024AA05372</h4>\n",
    "<h4>2. JANGALE SAVEDANA SUBHASH PRATIBHA - 2024AA05187</h4>\n",
    "<h4>3. GANAPATHY SUBRAMANIAN S - 2024AA05188</h4>\n",
    "<h4>4. ANANDAN A - 2024AA05269</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89617529-5b45-464b-af7a-f6cc75c9417e",
   "metadata": {},
   "source": [
    "<h3>Problem Statement</h3>\n",
    "<h4>Designing a Text Search and Query Correction System using Levenshtein Edit Distance algorithm for Medical Documents</h4`>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c89fe-74b2-4410-8897-f7f90efcfe66",
   "metadata": {},
   "source": [
    "# 1. Import and download the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef133da-8505-4397-af69-550d84a48ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saved\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saved\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saved\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document  # Must be imported!\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc2ce0-468d-42f4-a701-e3a2e02a3229",
   "metadata": {},
   "source": [
    "## Global variables and NLP setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8590ab6-9b02-4cfe-8cf7-d8e945391010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "inverted_index = defaultdict(set)\n",
    "all_terms = set()\n",
    "documents = []\n",
    "doc_metadata = []\n",
    "\n",
    "# NLP setup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97abe10-9444-4c1f-b5c1-3279e5e6e0a4",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "    a) Load documents from the directory provided.\n",
    "    b) Preprocess each document. Will display terms, unique terms and sample terms from each document.Remove all punctuation, numbers, and special    characters from the dataset. \n",
    "## 2. create non-positional inverted index in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8e0c716-70e3-4683-8cea-7d419b2794b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  User defined functions to remove all punctuation, numbers, and special characters from the dataset. \n",
    "#  Apply lemmatization techniques to convert words to their base or root forms.\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Full preprocessing with intermediate steps\"\"\"\n",
    "    print(\"\\n=== ORIGINAL TEXT (SAMPLE) ===\")\n",
    "    print(text[:200] + \"...\\n\" if len(text) > 200 else text)\n",
    "    \n",
    "    # 1. Clean text\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
    "    print(\"=== AFTER CLEANING ===\")\n",
    "    print(cleaned[:200] + \"...\\n\" if len(cleaned) > 200 else cleaned)\n",
    "    \n",
    "    # 2. Tokenization\n",
    "    tokens = word_tokenize(cleaned)\n",
    "    print(f\"TOKENS ({len(tokens)}):\", tokens[:30], \"...\\n\")\n",
    "    \n",
    "    # 3. Stopword removal\n",
    "    filtered = [w for w in tokens if w not in stop_words and len(w) > 2]\n",
    "    print(f\"AFTER STOPWORD REMOVAL ({len(filtered)}):\", filtered[:30], \"...\\n\")\n",
    "    \n",
    "    # 4. Lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in filtered]\n",
    "    print(f\"FINAL PROCESSED TERMS ({len(lemmatized)}):\", lemmatized[:30], \"...\")\n",
    "    \n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e18ffe0-edb5-4b04-9ad2-543784389e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined functions to read different types of files from a directory.\n",
    "def read_txt(file_path):\n",
    "    \"\"\"Read text file\"\"\"\n",
    "    encodings = ['utf-8', 'latin-1', 'windows-1252']\n",
    "    for encoding in encodings:\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "                return f.read()\n",
    "        \n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Read PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def read_csv(file_path):\n",
    "    \"\"\"Read CSV file\"\"\"\n",
    "    encodings = ['utf-8', 'latin-1', 'windows-1252']\n",
    "    for encoding in encodings:\n",
    "       df = pd.read_csv(file_path, encoding=encoding)\n",
    "       return ' '.join(df.select_dtypes(include=['object']).astype(str).values.flatten())\n",
    "\n",
    "def read_excel(file_path):\n",
    "    \"\"\"Read Excel file\"\"\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    return ' '.join(df.select_dtypes(include=['object']).astype(str).values.flatten())\n",
    "\n",
    "def read_docx(file_path):\n",
    "    \"\"\"Read Word DOCX file\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    return '\\n'.join([para.text for para in doc.paragraphs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "194cefb2-57f3-41ed-82e9-1bb896dcdb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory):\n",
    "    \"\"\"Load documents from directory and build index\"\"\"\n",
    "    global documents, doc_metadata, inverted_index, all_terms\n",
    "    document_metadata = []\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        raise FileNotFoundError(f\"Directory not found: {directory}\")\n",
    "    \n",
    "    print(f\"Loading documents from: {directory}\")\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                if file.endswith('.txt'):\n",
    "                    text = read_txt(file_path)\n",
    "                elif file.endswith('.pdf'):\n",
    "                    text = read_pdf(file_path)\n",
    "                elif file.endswith('.csv'):\n",
    "                    text = read_csv(file_path)\n",
    "                elif file.endswith(('.xls', '.xlsx')):\n",
    "                    text = read_excel(file_path)\n",
    "                elif file.endswith('.docx'):\n",
    "                    text = read_docx(file_path)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if text.strip():\n",
    "                    doc_id = len(documents)\n",
    "                    documents.append(text)\n",
    "                    doc_metadata.append({\n",
    "                        'file_name': file,\n",
    "                        'file_path': file_path\n",
    "                    })\n",
    "\n",
    "                    print(f\"\\n===============Loading: {file}=====================================\")\n",
    "                    # Add to index\n",
    "                    terms = preprocess_text(text)  # preprocessing each \n",
    "                    for term in terms:\n",
    "                        inverted_index[term].add(doc_id) # inverted index creation\n",
    "                        all_terms.add(term)\n",
    "\n",
    "                     # Store metadata - PROPERLY INDENTED\n",
    "                    document_metadata.append({\n",
    "                        'doc_id': doc_id,\n",
    "                        'filename': file,\n",
    "                        'filetype': os.path.splitext(file)[1],\n",
    "                        'terms': len(terms),\n",
    "                        'unique_terms': len(set(terms))\n",
    "                    })\n",
    "            \n",
    "                    # Display file processing info\n",
    "                    print(f\"\\n - {file} ({document_metadata[-1]['filetype']})\")\n",
    "                    print(f\"  - Total Terms: {document_metadata[-1]['terms']}\")\n",
    "                    print(f\"  - Unique terms: {document_metadata[-1]['unique_terms']}\")\n",
    "                    print(f\"  - Sample unique terms: {list(set(terms))[:5]}...\")\n",
    "                       \n",
    "                    print(f\"\\n Loaded: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "    print(f\"\\nTOTAL SUMMARY\")\n",
    "    print(f\"\\nTotal documents loaded: {len(documents)}\")\n",
    "    print(f\"Unique terms in index: {len(all_terms)}\")\n",
    "\n",
    "  #2)   # **********************Show most frequent terms - Sorted index creation******************************\n",
    "    top_terms = sorted(inverted_index.items(), \n",
    "                      key=lambda x: len(x[1]), \n",
    "                      reverse=True)[:5]\n",
    "\n",
    "    print(\"\\n**************Sorted index*****************\")\n",
    "    print(\"\\nTop 5 terms:\")\n",
    "    for term, doc_ids in top_terms:\n",
    "        print(f\"  {term}: appears in {(doc_ids)} documents\")\n",
    "\n",
    "    return inverted_index, document_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "367b28f4-db9e-4b7d-92ea-e1fa2cc536c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/AIML/IR/Assignment/medical_documents/\n",
      "\n",
      "===============OUTPUT=============================\n",
      "Loading documents from: D:\\AIML\\IR\\Assignment\\medical_documents\n",
      "\n",
      "===============Loading: Cardio.pdf=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks\n",
      "Pranav Rajpurkar\u0003PRANAVSR @CS.STANFORD .EDU\n",
      "Awni Y. Hannun\u0003AWNI @CS.STANFORD .EDU\n",
      "Masoumeh Haghpanahi MHAGHPANAHI @IRHYTHMTEC...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "cardiologistlevel arrhythmia detection with convolutional neural networks\n",
      "pranav rajpurkarpranavsr csstanford edu\n",
      "awni y hannunawni csstanford edu\n",
      "masoumeh haghpanahi mhaghpanahi irhythmtech com\n",
      "codie...\n",
      "\n",
      "TOKENS (4517): ['cardiologistlevel', 'arrhythmia', 'detection', 'with', 'convolutional', 'neural', 'networks', 'pranav', 'rajpurkarpranavsr', 'csstanford', 'edu', 'awni', 'y', 'hannunawni', 'csstanford', 'edu', 'masoumeh', 'haghpanahi', 'mhaghpanahi', 'irhythmtech', 'com', 'codie', 'bourn', 'cbourn', 'irhythmtech', 'com', 'andrew', 'y', 'ng', 'ang'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (2853): ['cardiologistlevel', 'arrhythmia', 'detection', 'convolutional', 'neural', 'networks', 'pranav', 'rajpurkarpranavsr', 'csstanford', 'edu', 'awni', 'hannunawni', 'csstanford', 'edu', 'masoumeh', 'haghpanahi', 'mhaghpanahi', 'irhythmtech', 'com', 'codie', 'bourn', 'cbourn', 'irhythmtech', 'com', 'andrew', 'ang', 'csstanford', 'edu', 'abstract', 'develop'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (2853): ['cardiologistlevel', 'arrhythmia', 'detection', 'convolutional', 'neural', 'network', 'pranav', 'rajpurkarpranavsr', 'csstanford', 'edu', 'awni', 'hannunawni', 'csstanford', 'edu', 'masoumeh', 'haghpanahi', 'mhaghpanahi', 'irhythmtech', 'com', 'codie', 'bourn', 'cbourn', 'irhythmtech', 'com', 'andrew', 'ang', 'csstanford', 'edu', 'abstract', 'develop'] ...\n",
      "\n",
      " - Cardio.pdf (.pdf)\n",
      "  - Total Terms: 2853\n",
      "  - Unique terms: 1264\n",
      "  - Sample unique terms: ['390', '4827', 'observation', 'challenge', 'problem']...\n",
      "\n",
      " Loaded: Cardio.pdf\n",
      "\n",
      "===============Loading: Cardiovascular  Pulmonary.txt=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "Cardiovascular / Pulmonary\n",
      "\n",
      "Sample Name: Acute Inferior Myocardial Infarction\n",
      "\n",
      "Description: Patient presents with a chief complaint of chest pain admitted to Coronary Care Unit due to acute inferior m...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "cardiovascular  pulmonary\n",
      "\n",
      "sample name acute inferior myocardial infarction\n",
      "\n",
      "description patient presents with a chief complaint of chest pain admitted to coronary care unit due to acute inferior myoc...\n",
      "\n",
      "TOKENS (628): ['cardiovascular', 'pulmonary', 'sample', 'name', 'acute', 'inferior', 'myocardial', 'infarction', 'description', 'patient', 'presents', 'with', 'a', 'chief', 'complaint', 'of', 'chest', 'pain', 'admitted', 'to', 'coronary', 'care', 'unit', 'due', 'to', 'acute', 'inferior', 'myocardial', 'infarction', 'medical'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (397): ['cardiovascular', 'pulmonary', 'sample', 'name', 'acute', 'inferior', 'myocardial', 'infarction', 'description', 'patient', 'presents', 'chief', 'complaint', 'chest', 'pain', 'admitted', 'coronary', 'care', 'unit', 'due', 'acute', 'inferior', 'myocardial', 'infarction', 'medical', 'transcription', 'sample', 'report', 'chief', 'complaint'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (397): ['cardiovascular', 'pulmonary', 'sample', 'name', 'acute', 'inferior', 'myocardial', 'infarction', 'description', 'patient', 'present', 'chief', 'complaint', 'chest', 'pain', 'admitted', 'coronary', 'care', 'unit', 'due', 'acute', 'inferior', 'myocardial', 'infarction', 'medical', 'transcription', 'sample', 'report', 'chief', 'complaint'] ...\n",
      "\n",
      " - Cardiovascular  Pulmonary.txt (.txt)\n",
      "  - Total Terms: 397\n",
      "  - Unique terms: 266\n",
      "  - Sample unique terms: ['nitroglycerine', 'cardiologist', 'reviewed', 'platelet', 'transcription']...\n",
      "\n",
      " Loaded: Cardiovascular  Pulmonary.txt\n",
      "\n",
      "===============Loading: DataAnalyticsinhealthcare.pdf=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/351792114\n",
      "Data Analytics in Healthcare Systems – Principles, Challenges, and\n",
      "Appli...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "see discussions st ats and author pr ofiles f or this public ation at  httpswww researchgatene tpublic ation351792114\n",
      "data analytics in healthcare systems  principles challenges and\n",
      "applications\n",
      "chapt...\n",
      "\n",
      "TOKENS (8391): ['see', 'discussions', 'st', 'ats', 'and', 'author', 'pr', 'ofiles', 'f', 'or', 'this', 'public', 'ation', 'at', 'httpswww', 'researchgatene', 'tpublic', 'ation351792114', 'data', 'analytics', 'in', 'healthcare', 'systems', 'principles', 'challenges', 'and', 'applications', 'chapt', 'er', 'may'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (5366): ['see', 'discussions', 'ats', 'author', 'ofiles', 'public', 'ation', 'httpswww', 'researchgatene', 'tpublic', 'ation351792114', 'data', 'analytics', 'healthcare', 'systems', 'principles', 'challenges', 'applications', 'chapt', 'may', '2021', 'doi', '10120197810031852461', 'citations', '2reads', '10647', 'author', 'sug', 'anthi', 'galg'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (5366): ['see', 'discussion', 'at', 'author', 'ofiles', 'public', 'ation', 'httpswww', 'researchgatene', 'tpublic', 'ation351792114', 'data', 'analytics', 'healthcare', 'system', 'principle', 'challenge', 'application', 'chapt', 'may', '2021', 'doi', '10120197810031852461', 'citation', '2reads', '10647', 'author', 'sug', 'anthi', 'galg'] ...\n",
      "\n",
      " - DataAnalyticsinhealthcare.pdf (.pdf)\n",
      "  - Total Terms: 5366\n",
      "  - Unique terms: 1843\n",
      "  - Sample unique terms: ['uplo', 'equipment', 'recommender', 'challenge', 'merging']...\n",
      "\n",
      " Loaded: DataAnalyticsinhealthcare.pdf\n",
      "\n",
      "===============Loading: gender-differences-arteries.pdf=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "Adrien Desjardins2\n",
      "1R o y a lF r e eH o s p i t a l ,L o n d o n ,U n i t e dK i n g d o m ;2University College\n",
      "London, London, United Kingdom\n",
      "BACKGROUND In situ fenestration (ISF) is an attractive op...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "adrien desjardins2\n",
      "1r o y a lf r e eh o s p i t a l l o n d o n u n i t e dk i n g d o m 2university college\n",
      "london london united kingdom\n",
      "background in situ fenestration isf is an attractive option to...\n",
      "\n",
      "TOKENS (1133): ['adrien', 'desjardins2', '1r', 'o', 'y', 'a', 'lf', 'r', 'e', 'eh', 'o', 's', 'p', 'i', 't', 'a', 'l', 'l', 'o', 'n', 'd', 'o', 'n', 'u', 'n', 'i', 't', 'e', 'dk', 'i'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (666): ['adrien', 'desjardins2', '2university', 'college', 'london', 'london', 'united', 'kingdom', 'background', 'situ', 'fenestration', 'isf', 'attractive', 'option', 'preserve', 'aortic', 'branch', 'patency', 'fenestrated', 'endovascular', 'aorticrepair', 'fevar', 'complex', 'aortic', 'aneurysms', 'although', 'prefenestrated', 'grafts', 'suitable', 'common'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (666): ['adrien', 'desjardins2', '2university', 'college', 'london', 'london', 'united', 'kingdom', 'background', 'situ', 'fenestration', 'isf', 'attractive', 'option', 'preserve', 'aortic', 'branch', 'patency', 'fenestrated', 'endovascular', 'aorticrepair', 'fevar', 'complex', 'aortic', 'aneurysm', 'although', 'prefenestrated', 'graft', 'suitable', 'common'] ...\n",
      "\n",
      " - gender-differences-arteries.pdf (.pdf)\n",
      "  - Total Terms: 666\n",
      "  - Unique terms: 467\n",
      "  - Sample unique terms: ['arte', 'siroli', 'clinical', 'still', 'formulation']...\n",
      "\n",
      " Loaded: gender-differences-arteries.pdf\n",
      "\n",
      "===============Loading: in-hospital-mortality-trends-by-health-category.csv=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "05/2018 Anxiety Ambulatory Surgery 09/2018 Anxiety Ambulatory Surgery 10/2018 Anxiety Ambulatory Surgery 01/2019 Anxiety Ambulatory Surgery 06/2019 Anxiety Ambulatory Surgery 02/2020 Anxiety Ambulator...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "052018 anxiety ambulatory surgery 092018 anxiety ambulatory surgery 102018 anxiety ambulatory surgery 012019 anxiety ambulatory surgery 062019 anxiety ambulatory surgery 022020 anxiety ambulatory surg...\n",
      "\n",
      "TOKENS (10907): ['052018', 'anxiety', 'ambulatory', 'surgery', '092018', 'anxiety', 'ambulatory', 'surgery', '102018', 'anxiety', 'ambulatory', 'surgery', '012019', 'anxiety', 'ambulatory', 'surgery', '062019', 'anxiety', 'ambulatory', 'surgery', '022020', 'anxiety', 'ambulatory', 'surgery', '032020', 'anxiety', 'ambulatory', 'surgery', '042020', 'anxiety'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (10907): ['052018', 'anxiety', 'ambulatory', 'surgery', '092018', 'anxiety', 'ambulatory', 'surgery', '102018', 'anxiety', 'ambulatory', 'surgery', '012019', 'anxiety', 'ambulatory', 'surgery', '062019', 'anxiety', 'ambulatory', 'surgery', '022020', 'anxiety', 'ambulatory', 'surgery', '032020', 'anxiety', 'ambulatory', 'surgery', '042020', 'anxiety'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (10907): ['052018', 'anxiety', 'ambulatory', 'surgery', '092018', 'anxiety', 'ambulatory', 'surgery', '102018', 'anxiety', 'ambulatory', 'surgery', '012019', 'anxiety', 'ambulatory', 'surgery', '062019', 'anxiety', 'ambulatory', 'surgery', '022020', 'anxiety', 'ambulatory', 'surgery', '032020', 'anxiety', 'ambulatory', 'surgery', '042020', 'anxiety'] ...\n",
      "\n",
      " - in-hospital-mortality-trends-by-health-category.csv (.csv)\n",
      "  - Total Terms: 10907\n",
      "  - Unique terms: 123\n",
      "  - Sample unique terms: ['92019', '112018', '102021', '62020', '62021']...\n",
      "\n",
      " Loaded: in-hospital-mortality-trends-by-health-category.csv\n",
      "\n",
      "===============Loading: Medical Specialty.txt=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "Medical Specialty:\n",
      "Cardiovascular / Pulmonary\n",
      "\n",
      "Sample Name: Abnormal Echocardiogram\n",
      "\n",
      "Description: Abnormal echocardiogram findings and followup. Shortness of breath, congestive heart failure, and valv...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "medical specialty\n",
      "cardiovascular  pulmonary\n",
      "\n",
      "sample name abnormal echocardiogram\n",
      "\n",
      "description abnormal echocardiogram findings and followup shortness of breath congestive heart failure and valvular in...\n",
      "\n",
      "TOKENS (567): ['medical', 'specialty', 'cardiovascular', 'pulmonary', 'sample', 'name', 'abnormal', 'echocardiogram', 'description', 'abnormal', 'echocardiogram', 'findings', 'and', 'followup', 'shortness', 'of', 'breath', 'congestive', 'heart', 'failure', 'and', 'valvular', 'insufficiency', 'the', 'patient', 'complains', 'of', 'shortness', 'of', 'breath'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (379): ['medical', 'specialty', 'cardiovascular', 'pulmonary', 'sample', 'name', 'abnormal', 'echocardiogram', 'description', 'abnormal', 'echocardiogram', 'findings', 'followup', 'shortness', 'breath', 'congestive', 'heart', 'failure', 'valvular', 'insufficiency', 'patient', 'complains', 'shortness', 'breath', 'worsening', 'patient', 'underwent', 'echocardiogram', 'shows', 'severe'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (379): ['medical', 'specialty', 'cardiovascular', 'pulmonary', 'sample', 'name', 'abnormal', 'echocardiogram', 'description', 'abnormal', 'echocardiogram', 'finding', 'followup', 'shortness', 'breath', 'congestive', 'heart', 'failure', 'valvular', 'insufficiency', 'patient', 'complains', 'shortness', 'breath', 'worsening', 'patient', 'underwent', 'echocardiogram', 'show', 'severe'] ...\n",
      "\n",
      " - Medical Specialty.txt (.txt)\n",
      "  - Total Terms: 379\n",
      "  - Unique terms: 237\n",
      "  - Sample unique terms: ['ventricular', 'systolic', 'reviewed', 'reason', 'atraumatic']...\n",
      "\n",
      " Loaded: Medical Specialty.txt\n",
      "\n",
      "===============Loading: Medical Specialty_Gastro.pdf=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "Medical Specialty:  \n",
      "Gastroenterology  \n",
      " \n",
      "Sample Name:  Colonoscopy & Polypectomy - 3 \n",
      " \n",
      "Description:  Total colonoscopy with biopsy and snare polypectomy.  \n",
      "(Medical Transcription Sample Report)  \n",
      "PR...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "medical specialty  \n",
      "gastroenterology  \n",
      " \n",
      "sample name  colonoscopy  polypectomy  3 \n",
      " \n",
      "description  total colonoscopy with biopsy and snare polypectomy  \n",
      "medical transcription sample report  \n",
      "preoperati...\n",
      "\n",
      "TOKENS (336): ['medical', 'specialty', 'gastroenterology', 'sample', 'name', 'colonoscopy', 'polypectomy', '3', 'description', 'total', 'colonoscopy', 'with', 'biopsy', 'and', 'snare', 'polypectomy', 'medical', 'transcription', 'sample', 'report', 'preoperative', 'diagnosis', 'alternating', 'hard', 'and', 'soft', 'stools', 'postoperative', 'diagnosis', 'sigmoid'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (209): ['medical', 'specialty', 'gastroenterology', 'sample', 'name', 'colonoscopy', 'polypectomy', 'description', 'total', 'colonoscopy', 'biopsy', 'snare', 'polypectomy', 'medical', 'transcription', 'sample', 'report', 'preoperative', 'diagnosis', 'alternating', 'hard', 'soft', 'stools', 'postoperative', 'diagnosis', 'sigmoid', 'diverticulosis', 'sessile', 'polyp', 'sigmoid'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (209): ['medical', 'specialty', 'gastroenterology', 'sample', 'name', 'colonoscopy', 'polypectomy', 'description', 'total', 'colonoscopy', 'biopsy', 'snare', 'polypectomy', 'medical', 'transcription', 'sample', 'report', 'preoperative', 'diagnosis', 'alternating', 'hard', 'soft', 'stool', 'postoperative', 'diagnosis', 'sigmoid', 'diverticulosis', 'sessile', 'polyp', 'sigmoid'] ...\n",
      "\n",
      " - Medical Specialty_Gastro.pdf (.pdf)\n",
      "  - Total Terms: 209\n",
      "  - Unique terms: 132\n",
      "  - Sample unique terms: ['reaching', 'ileo', 'transcription', 'approximately', 'assessment']...\n",
      "\n",
      " Loaded: Medical Specialty_Gastro.pdf\n",
      "\n",
      "===============Loading: Medical_history.docx=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "Medical Specialty:\n",
      "Surgery\n",
      "\n",
      "Sample Name: Arthroscopy & Chondroplasty\n",
      "\n",
      "Description: Diagnostic arthroscopy with partial chondroplasty of patella, lateral retinacular release, and open tibial tubercle t...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "medical specialty\n",
      "surgery\n",
      "\n",
      "sample name arthroscopy  chondroplasty\n",
      "\n",
      "description diagnostic arthroscopy with partial chondroplasty of patella lateral retinacular release and open tibial tubercle transfe...\n",
      "\n",
      "TOKENS (716): ['medical', 'specialty', 'surgery', 'sample', 'name', 'arthroscopy', 'chondroplasty', 'description', 'diagnostic', 'arthroscopy', 'with', 'partial', 'chondroplasty', 'of', 'patella', 'lateral', 'retinacular', 'release', 'and', 'open', 'tibial', 'tubercle', 'transfer', 'with', 'fixation', 'of', 'two', '45', 'mm', 'cannulated'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (417): ['medical', 'specialty', 'surgery', 'sample', 'name', 'arthroscopy', 'chondroplasty', 'description', 'diagnostic', 'arthroscopy', 'partial', 'chondroplasty', 'patella', 'lateral', 'retinacular', 'release', 'open', 'tibial', 'tubercle', 'transfer', 'fixation', 'two', 'cannulated', 'screws', 'gradeiv', 'chondromalacia', 'patella', 'patellofemoral', 'malalignment', 'syndrome'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (417): ['medical', 'specialty', 'surgery', 'sample', 'name', 'arthroscopy', 'chondroplasty', 'description', 'diagnostic', 'arthroscopy', 'partial', 'chondroplasty', 'patella', 'lateral', 'retinacular', 'release', 'open', 'tibial', 'tubercle', 'transfer', 'fixation', 'two', 'cannulated', 'screw', 'gradeiv', 'chondromalacia', 'patella', 'patellofemoral', 'malalignment', 'syndrome'] ...\n",
      "\n",
      " - Medical_history.docx (.docx)\n",
      "  - Total Terms: 417\n",
      "  - Unique terms: 249\n",
      "  - Sample unique terms: ['drilled', 'abcd', '325', 'transcription', 'approximately']...\n",
      "\n",
      " Loaded: Medical_history.docx\n",
      "\n",
      "===============Loading: mtsamples.csv=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      " A 23-year-old white female presents with complaint of allergies.  Allergy / Immunology  Allergic Rhinitis  SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used ...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      " a 23yearold white female presents with complaint of allergies  allergy  immunology  allergic rhinitis  subjective  this 23yearold white female presents with complaint of allergies  she used to have a...\n",
      "\n",
      "TOKENS (68702): ['a', '23yearold', 'white', 'female', 'presents', 'with', 'complaint', 'of', 'allergies', 'allergy', 'immunology', 'allergic', 'rhinitis', 'subjective', 'this', '23yearold', 'white', 'female', 'presents', 'with', 'complaint', 'of', 'allergies', 'she', 'used', 'to', 'have', 'allergies', 'when', 'she'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (49840): ['23yearold', 'white', 'female', 'presents', 'complaint', 'allergies', 'allergy', 'immunology', 'allergic', 'rhinitis', 'subjective', '23yearold', 'white', 'female', 'presents', 'complaint', 'allergies', 'used', 'allergies', 'lived', 'seattle', 'thinks', 'worse', 'past', 'tried', 'claritin', 'zyrtec', 'worked', 'short', 'time'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (49840): ['23yearold', 'white', 'female', 'present', 'complaint', 'allergy', 'allergy', 'immunology', 'allergic', 'rhinitis', 'subjective', '23yearold', 'white', 'female', 'present', 'complaint', 'allergy', 'used', 'allergy', 'lived', 'seattle', 'think', 'worse', 'past', 'tried', 'claritin', 'zyrtec', 'worked', 'short', 'time'] ...\n",
      "\n",
      " - mtsamples.csv (.csv)\n",
      "  - Total Terms: 49840\n",
      "  - Unique terms: 4646\n",
      "  - Sample unique terms: ['tsh', 'observation', 'hydrochlorothiazide', 'trigone', 'ethibond']...\n",
      "\n",
      " Loaded: mtsamples.csv\n",
      "\n",
      "===============Loading: mtsamples.xlsx=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      " A 23-year-old white female presents with complaint of allergies.  Allergy / Immunology  Allergic Rhinitis  SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used ...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      " a 23yearold white female presents with complaint of allergies  allergy  immunology  allergic rhinitis  subjective  this 23yearold white female presents with complaint of allergies  she used to have a...\n",
      "\n",
      "TOKENS (44892): ['a', '23yearold', 'white', 'female', 'presents', 'with', 'complaint', 'of', 'allergies', 'allergy', 'immunology', 'allergic', 'rhinitis', 'subjective', 'this', '23yearold', 'white', 'female', 'presents', 'with', 'complaint', 'of', 'allergies', 'she', 'used', 'to', 'have', 'allergies', 'when', 'she'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (26030): ['23yearold', 'white', 'female', 'presents', 'complaint', 'allergies', 'allergy', 'immunology', 'allergic', 'rhinitis', 'subjective', '23yearold', 'white', 'female', 'presents', 'complaint', 'allergies', 'used', 'allergies', 'lived', 'seattle', 'thinks', 'worse', 'past', 'tried', 'claritin', 'zyrtec', 'worked', 'short', 'time'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (26030): ['23yearold', 'white', 'female', 'present', 'complaint', 'allergy', 'allergy', 'immunology', 'allergic', 'rhinitis', 'subjective', '23yearold', 'white', 'female', 'present', 'complaint', 'allergy', 'used', 'allergy', 'lived', 'seattle', 'think', 'worse', 'past', 'tried', 'claritin', 'zyrtec', 'worked', 'short', 'time'] ...\n",
      "\n",
      " - mtsamples.xlsx (.xlsx)\n",
      "  - Total Terms: 26030\n",
      "  - Unique terms: 4647\n",
      "  - Sample unique terms: ['tsh', 'observation', 'hydrochlorothiazide', 'trigone', 'ethibond']...\n",
      "\n",
      " Loaded: mtsamples.xlsx\n",
      "\n",
      "===============Loading: train.txt=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "4\tCatheterization laboratory events and hospital outcome with direct angioplasty for acute myocardial infarction To assess the safety of direct infarct angioplasty without antecedent thrombolytic ther...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "4\tcatheterization laboratory events and hospital outcome with direct angioplasty for acute myocardial infarction to assess the safety of direct infarct angioplasty without antecedent thrombolytic ther...\n",
      "\n",
      "TOKENS (2157): ['4', 'catheterization', 'laboratory', 'events', 'and', 'hospital', 'outcome', 'with', 'direct', 'angioplasty', 'for', 'acute', 'myocardial', 'infarction', 'to', 'assess', 'the', 'safety', 'of', 'direct', 'infarct', 'angioplasty', 'without', 'antecedent', 'thrombolytic', 'therapy', 'catheterization', 'laboratory', 'and', 'hospital'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (1255): ['catheterization', 'laboratory', 'events', 'hospital', 'outcome', 'direct', 'angioplasty', 'acute', 'myocardial', 'infarction', 'assess', 'safety', 'direct', 'infarct', 'angioplasty', 'without', 'antecedent', 'thrombolytic', 'therapy', 'catheterization', 'laboratory', 'hospital', 'events', 'assessed', 'consecutively', 'treated', 'patients', 'infarctions', 'involving', 'left'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (1255): ['catheterization', 'laboratory', 'event', 'hospital', 'outcome', 'direct', 'angioplasty', 'acute', 'myocardial', 'infarction', 'assess', 'safety', 'direct', 'infarct', 'angioplasty', 'without', 'antecedent', 'thrombolytic', 'therapy', 'catheterization', 'laboratory', 'hospital', 'event', 'assessed', 'consecutively', 'treated', 'patient', 'infarction', 'involving', 'left'] ...\n",
      "\n",
      " - train.txt (.txt)\n",
      "  - Total Terms: 1255\n",
      "  - Unique terms: 629\n",
      "  - Sample unique terms: ['twentynine', 'clinical', 'nature', 'terminal', 'problem']...\n",
      "\n",
      " Loaded: train.txt\n",
      "\n",
      "===============Loading: Train_Data.txt=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "###24293578\n",
      "OBJECTIVE\tTo investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the ef...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "24293578\n",
      "objective\tto investigate the efficacy of 6 weeks of daily lowdose oral prednisolone in improving pain  mobility  and systemic lowgrade inflammation in the short term and whether the effect wo...\n",
      "\n",
      "TOKENS (5539): ['24293578', 'objective', 'to', 'investigate', 'the', 'efficacy', 'of', '6', 'weeks', 'of', 'daily', 'lowdose', 'oral', 'prednisolone', 'in', 'improving', 'pain', 'mobility', 'and', 'systemic', 'lowgrade', 'inflammation', 'in', 'the', 'short', 'term', 'and', 'whether', 'the', 'effect'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (3453): ['24293578', 'objective', 'investigate', 'efficacy', 'weeks', 'daily', 'lowdose', 'oral', 'prednisolone', 'improving', 'pain', 'mobility', 'systemic', 'lowgrade', 'inflammation', 'short', 'term', 'whether', 'effect', 'would', 'sustained', 'weeks', 'older', 'adults', 'moderate', 'severe', 'knee', 'osteoarthritis', 'methods', 'total'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (3453): ['24293578', 'objective', 'investigate', 'efficacy', 'week', 'daily', 'lowdose', 'oral', 'prednisolone', 'improving', 'pain', 'mobility', 'systemic', 'lowgrade', 'inflammation', 'short', 'term', 'whether', 'effect', 'would', 'sustained', 'week', 'older', 'adult', 'moderate', 'severe', 'knee', 'osteoarthritis', 'method', 'total'] ...\n",
      "\n",
      " - Train_Data.txt (.txt)\n",
      "  - Total Terms: 3453\n",
      "  - Unique terms: 1344\n",
      "  - Sample unique terms: ['challenge', 'problem', 'joint', 'formulation', 'monetary']...\n",
      "\n",
      " Loaded: Train_Data.txt\n",
      "\n",
      "TOTAL SUMMARY\n",
      "\n",
      "Total documents loaded: 12\n",
      "Unique terms in index: 8116\n",
      "\n",
      "**************Sorted index*****************\n",
      "\n",
      "Top 5 terms:\n",
      "  patient: appears in {0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11} documents\n",
      "  disease: appears in {0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11} documents\n",
      "  risk: appears in {1, 2, 3, 5, 6, 7, 8, 9, 10, 11} documents\n",
      "  heart: appears in {0, 1, 2, 3, 5, 8, 9, 10, 11} documents\n",
      "  also: appears in {0, 2, 3, 5, 7, 8, 9, 10, 11} documents\n"
     ]
    }
   ],
   "source": [
    " # Load documents\n",
    "directory = \"D:/AIML/IR/Assignment/medical_documents/\"\n",
    "print(directory)\n",
    "path = os.path.abspath(directory)\n",
    "print(f\"\\n===============OUTPUT=============================\")\n",
    "inverted_index, document_metadata  =  load_documents(path) #load all the document from directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f9132-365c-4644-b647-83ce06bca934",
   "metadata": {},
   "source": [
    "**Justification:**\n",
    "\n",
    "1) Above function first load all the documents with different file extensions from the given directory\n",
    "2) Preprocessing is done for each document.\n",
    "    a) ORIGINAL TEXT (SAMPLE) shows few line from the document\n",
    "    b) AFTER CLEANING shows text after removing special charaters, then converting all text to lowercase\n",
    "    c) TOKENS (stream of text converted into smaller units called tokens) are extracted from each document\n",
    "    d) AFTER STOPWORD REMOVAL removes all the stopwords(common English words to be excluded from each document\n",
    "    e) FINAL PROCESSED TERMS shows all the words after Lemmatization(reduce words to their base or dictionary form).\n",
    "\n",
    "3) Sorted index is created after preprocessing step. All the indexed are sorted and top 5 terms are displayed appearing in respective documents\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Preprocess text for improving search efficiency with efficient indexing, accuracy, and relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b09fe-7647-4c34-a487-3439484e9662",
   "metadata": {},
   "source": [
    "### 3. Wildcard search and regular search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74051cdf-3965-48dc-8f57-cd4d6a452fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Functions\n",
    "def wildcard_search(query, inverted_index):\n",
    "    if not query.endswith('*'):\n",
    "        return []\n",
    "    prefix = query[:-1].lower()\n",
    "    return sorted([term for term in inverted_index.keys() \n",
    "                 if term.startswith(prefix)])\n",
    "\n",
    "def regular_search(query, inverted_index, doc_metadata):\n",
    "    terms = preprocess_text(query)\n",
    "    if not terms:\n",
    "        return []\n",
    "    \n",
    "    # Find documents containing ALL terms (AND logic)\n",
    "    matching_docs = None\n",
    "    for term in terms:\n",
    "        if term in inverted_index:\n",
    "            if matching_docs is None:\n",
    "                matching_docs = set(inverted_index[term])\n",
    "            else:\n",
    "                matching_docs.intersection_update(inverted_index[term])\n",
    "        else:\n",
    "            return []  # If any term doesn't exist, return nothing\n",
    "    \n",
    "    return list(matching_docs) if matching_docs else []\n",
    "\n",
    "def search(query, inverted_index, doc_metadata):\n",
    "    if query.endswith('*'):\n",
    "        terms = wildcard_search(query, inverted_index)\n",
    "        # For wildcard searches, return terms with document counts\n",
    "        enriched_terms = []\n",
    "        for term in terms:\n",
    "            doc_count = len(inverted_index.get(term, []))\n",
    "            enriched_terms.append({\n",
    "                'term': term,\n",
    "                'doc_count': doc_count,\n",
    "                'example_docs': list(inverted_index.get(term, []))[:3]  # Show first 3 docs\n",
    "            })\n",
    "        return {\n",
    "            'type': 'wildcard',\n",
    "            'query': query,\n",
    "            'count': len(terms),\n",
    "            'results': enriched_terms  \n",
    "        }\n",
    "    else:\n",
    "        doc_ids = regular_search(query, inverted_index, doc_metadata)\n",
    "        results = []\n",
    "        for doc_id in doc_ids:\n",
    "            doc = doc_metadata[doc_id]\n",
    "            results.append({\n",
    "                'doc_id': doc_id,\n",
    "                'filename': doc['filename']\n",
    "            })\n",
    "        return {\n",
    "            'type': 'regular', \n",
    "            'query': query,\n",
    "            'count': len(results),\n",
    "            'results': results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c1a45fe-a0cf-49d5-b2bf-3e74907f5865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***************TESTING SEARCHES**********************\n",
      "\n",
      "Search options:\n",
      "- Regular search: 'eg: diabetes'\n",
      "- Wildcard search: 'eg: cardio*'\n",
      "Type 'exit' to quit\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Search term:  cardio*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- cardio (in 2 documents)\n",
      "- cardiogenic (in 1 documents)\n",
      "- cardiographic (in 1 documents)\n",
      "- cardiol (in 1 documents)\n",
      "- cardiolo (in 1 documents)\n",
      "- cardiologist (in 2 documents)\n",
      "- cardiologistlevel (in 1 documents)\n",
      "- cardiology (in 4 documents)\n",
      "- cardiopulmonary (in 4 documents)\n",
      "- cardiovascular (in 9 documents)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Search term:  patient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "patient\n",
      "=== AFTER CLEANING ===\n",
      "patient\n",
      "TOKENS (1): ['patient'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (1): ['patient'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (1): ['patient'] ...\n",
      "- Document: Cardio.pdf\n",
      "- Document: Cardiovascular  Pulmonary.txt\n",
      "- Document: DataAnalyticsinhealthcare.pdf\n",
      "- Document: gender-differences-arteries.pdf\n",
      "- Document: Medical Specialty.txt\n",
      "- Document: Medical Specialty_Gastro.pdf\n",
      "- Document: Medical_history.docx\n",
      "- Document: mtsamples.csv\n",
      "- Document: mtsamples.xlsx\n",
      "- Document: train.txt\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Search term:  exit\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n***************TESTING SEARCHES**********************\")\n",
    "print(\"\\nSearch options:\")\n",
    "print(\"- Regular search: 'eg: diabetes'\")\n",
    "print(\"- Wildcard search: 'eg: cardio*'\")\n",
    "print(\"Type 'exit' to quit\\n\")\n",
    "         \n",
    "while True:\n",
    "   \n",
    "    query = input(\"\\nEnter Search term: \").strip()\n",
    "\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    " \n",
    "    results = search(query, inverted_index, document_metadata)\n",
    "    \n",
    "    if results['type'] == 'wildcard':\n",
    "        # Wildcard search results\n",
    "            if results['count'] > 0:\n",
    "                for term_info in results['results'][:10]:\n",
    "                    print(f\"- {term_info['term']} (in {term_info['doc_count']} documents)\")\n",
    "            else:\n",
    "                print(\"\\nNo matching terms found\")  # Wildcard-specific no-results message\n",
    "    \n",
    "    else:\n",
    "            # Regular search results\n",
    "            if results['count'] > 0:\n",
    "                for doc in results['results'][:10]:\n",
    "                    print(f\"- Document: {doc['filename']}\")\n",
    "            else:\n",
    "                print(\"\\nNo direct matches found\")  # ✅ Only shows when count == 0\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d3b27-f287-41a3-ac52-bd498b06aca8",
   "metadata": {},
   "source": [
    "### 4.  Levenshtein distance logic and suggest terms for misspelled search strings based on distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8e21681-31bf-42cc-9c00-011139b0956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "    \n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    \n",
    "    prev_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        curr_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            inserts = prev_row[j + 1] + 1\n",
    "            deletes = curr_row[j] + 1\n",
    "            substitute = prev_row[j] + (c1 != c2)\n",
    "            curr_row.append(min(inserts, deletes, substitute))\n",
    "        prev_row = curr_row\n",
    "    \n",
    "    return prev_row[-1]\n",
    "\n",
    "def suggest_terms(misspelled_word, inverted_index, max_suggestions=5):\n",
    "    # First check for quick matches with common errors\n",
    "    suggestions = []\n",
    "    \n",
    "    # Calculate distances to all terms in our vocabulary\n",
    "    distances = []\n",
    "    for correct_word in inverted_index.keys():\n",
    "        distance = levenshtein(misspelled_word.lower(), correct_word.lower())\n",
    "        distances.append((correct_word, distance))\n",
    "    \n",
    "    # Sort by distance (closest first)\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Get the top N suggestions with smallest distance\n",
    "    closest_matches = [word for word, dist in distances[:max_suggestions]]\n",
    "    \n",
    "    return closest_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc3746-f6cb-4d49-97f8-ac61cdf2a18c",
   "metadata": {},
   "source": [
    "**Justification:**\n",
    "\n",
    "Above functions are Levenshtein distance calculation. The Levenshtein distance (or edit distance) measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into another. \n",
    "\n",
    "Second function suggest_terms Used to find near matches for misspelled terms using levenshtein function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f981035-2301-407d-8c0a-073be83ff16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **********OUTPUT*********\n",
      "\n",
      "Type Mispelled word: 'eg: cardeo'\n",
      "Type 'exit' to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Search term:  dybetes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Did you mean:\n",
      "'dybetes': ['diabetes', 'detec', 'better', 'deep', 'detect']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Search term:  exit\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n **********OUTPUT*********\")\n",
    "print(\"\\nType Mispelled word: 'eg: cardeo'\")\n",
    "print(\"Type 'exit' to quit\")\n",
    "while True:\n",
    "            query = input(\"\\nEnter Search term: \").strip()\n",
    "            \n",
    "            if query.lower() == 'exit':\n",
    "                break  # Exit the loop before processing\n",
    "        \n",
    "            print(\"\\nDid you mean:\")\n",
    "            suggestions = suggest_terms(query, inverted_index)\n",
    "            print(f\"'{query}': {suggestions}\")\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6852c29-e22f-4ffb-8e55-4dd83a3856e2",
   "metadata": {},
   "source": [
    "**Justification**\n",
    "Above functions are created when user wants to search terms.\n",
    "\n",
    "***Purpose of wildcard search:***\n",
    "Enables prefix-based searching (e.g., \"cardio*\" finds \"cardiovascular\", \"cardiology\")\n",
    "Supports exploratory searches when users know only the beginning of terms\n",
    "\n",
    "***Purpose of regular search:***\n",
    "Performs exact term matching with AND logic\n",
    "Handles preprocessed queries (tokenized, normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
