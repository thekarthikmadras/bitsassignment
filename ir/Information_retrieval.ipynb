{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f8abe90-cabe-44de-a7c7-c8eee462936f",
   "metadata": {},
   "source": [
    "<h2>Information Retrieval Assignment 1</h2>\n",
    "<h4>Group ID: 26</h4>\n",
    "<h4>Group Members Name with Student ID:</h4>\n",
    "<h4>1. KARTHIKEYAN J - 2024AA05372</h4>\n",
    "<h4>2. JANGALE SAVEDANA SUBHASH PRATIBHA - 2024AA05187</h4>\n",
    "<h4>3. GANAPATHY SUBRAMANIAN S - 2024AA05188</h4>\n",
    "<h4>4. ANANDAN A - 2024AA05269</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89617529-5b45-464b-af7a-f6cc75c9417e",
   "metadata": {},
   "source": [
    "<h3>Problem Statement</h3>\n",
    "<h4>Designing a Text Search and Query Correction System using Levenshtein Edit Distance algorithm for Medical Documents</h4`>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c89fe-74b2-4410-8897-f7f90efcfe66",
   "metadata": {},
   "source": [
    "# 1. Import and download the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ef133da-8505-4397-af69-550d84a48ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saved\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saved\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saved\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from docx import Document  # Must be imported!\n",
    "import os\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc2ce0-468d-42f4-a701-e3a2e02a3229",
   "metadata": {},
   "source": [
    "## Global variables and NLP setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8590ab6-9b02-4cfe-8cf7-d8e945391010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "inverted_index = defaultdict(set)\n",
    "all_terms = set()\n",
    "documents = []\n",
    "doc_metadata = []\n",
    "\n",
    "# NLP setup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97abe10-9444-4c1f-b5c1-3279e5e6e0a4",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "### a) User defined functions to remove all punctuation, numbers, and special characters from the dataset. \n",
    "### b) Apply lemmatization techniques to convert words to their base or root forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8e0c716-70e3-4683-8cea-7d419b2794b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Full preprocessing with intermediate steps\"\"\"\n",
    "    print(\"\\n=== ORIGINAL TEXT (SAMPLE) ===\")\n",
    "    print(text[:200] + \"...\\n\" if len(text) > 200 else text)\n",
    "    \n",
    "    # 1. Clean text\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
    "    print(\"=== AFTER CLEANING ===\")\n",
    "    print(cleaned[:200] + \"...\\n\" if len(cleaned) > 200 else cleaned)\n",
    "    \n",
    "    # 2. Tokenization\n",
    "    tokens = word_tokenize(cleaned)\n",
    "    print(f\"TOKENS ({len(tokens)}):\", tokens[:30], \"...\\n\")\n",
    "    \n",
    "    # 3. Stopword removal\n",
    "    filtered = [w for w in tokens if w not in stop_words and len(w) > 2]\n",
    "    print(f\"AFTER STOPWORD REMOVAL ({len(filtered)}):\", filtered[:30], \"...\\n\")\n",
    "    \n",
    "    # 4. Lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in filtered]\n",
    "    print(f\"FINAL PROCESSED TERMS ({len(lemmatized)}):\", lemmatized[:30], \"...\")\n",
    "    \n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca47e2-6307-4658-965f-099ceb3af037",
   "metadata": {},
   "source": [
    "### User defined functions to read different types of files from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e18ffe0-edb5-4b04-9ad2-543784389e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_path):\n",
    "    \"\"\"Read text file\"\"\"\n",
    "    encodings = ['utf-8', 'latin-1', 'windows-1252']\n",
    "    for encoding in encodings:\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "                return f.read()\n",
    "        \n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Read PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def read_csv(file_path):\n",
    "    \"\"\"Read CSV file\"\"\"\n",
    "    encodings = ['utf-8', 'latin-1', 'windows-1252']\n",
    "    for encoding in encodings:\n",
    "       df = pd.read_csv(file_path, encoding=encoding)\n",
    "       return ' '.join(df.select_dtypes(include=['object']).astype(str).values.flatten())\n",
    "\n",
    "def read_excel(file_path):\n",
    "    \"\"\"Read Excel file\"\"\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    return ' '.join(df.select_dtypes(include=['object']).astype(str).values.flatten())\n",
    "\n",
    "def read_docx(file_path):\n",
    "    \"\"\"Read Word DOCX file\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    return '\\n'.join([para.text for para in doc.paragraphs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c8baf-2040-4799-9e4f-240d25cd2926",
   "metadata": {},
   "source": [
    "### a) Load documents from the directory provided.\n",
    "### b) Preprocess each document.\n",
    "### c) create non-positional inverted index. Will display terms, unique terms and sample terms from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "194cefb2-57f3-41ed-82e9-1bb896dcdb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory):\n",
    "    \"\"\"Load documents from directory and build index\"\"\"\n",
    "    global documents, doc_metadata, inverted_index, all_terms\n",
    "    document_metadata = []\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        raise FileNotFoundError(f\"Directory not found: {directory}\")\n",
    "    \n",
    "    print(f\"Loading documents from: {directory}\")\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                if file.endswith('.txt'):\n",
    "                    text = read_txt(file_path)\n",
    "                elif file.endswith('.pdf'):\n",
    "                    text = read_pdf(file_path)\n",
    "                elif file.endswith('.csv'):\n",
    "                    text = read_csv(file_path)\n",
    "                elif file.endswith(('.xls', '.xlsx')):\n",
    "                    text = read_excel(file_path)\n",
    "                elif file.endswith('.docx'):\n",
    "                    text = read_docx(file_path)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if text.strip():\n",
    "                    doc_id = len(documents)\n",
    "                    documents.append(text)\n",
    "                    doc_metadata.append({\n",
    "                        'file_name': file,\n",
    "                        'file_path': file_path\n",
    "                    })\n",
    "\n",
    "                    print(f\"\\n===============Loading: {file}=====================================\")\n",
    "                    # Add to index\n",
    "                    terms = preprocess_text(text)  # preprocessing each \n",
    "                    for term in terms:\n",
    "                        inverted_index[term].add(doc_id) # inverted index creation\n",
    "                        all_terms.add(term)\n",
    "\n",
    "                     # Store metadata - PROPERLY INDENTED\n",
    "                    document_metadata.append({\n",
    "                        'doc_id': doc_id,\n",
    "                        'filename': file,\n",
    "                        'filetype': os.path.splitext(file)[1],\n",
    "                        'terms': len(terms),\n",
    "                        'unique_terms': len(set(terms))\n",
    "                    })\n",
    "            \n",
    "                    # Display file processing info\n",
    "                    print(f\"\\nðŸ“„ {file} ({document_metadata[-1]['filetype']})\")\n",
    "                    print(f\"  - Terms: {document_metadata[-1]['terms']}\")\n",
    "                    print(f\"  - Unique terms: {document_metadata[-1]['unique_terms']}\")\n",
    "                    print(f\"  - Sample terms: {list(set(terms))[:5]}...\")\n",
    "                       \n",
    "                    print(f\"Loaded: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "    print(f\"\\nTOTAL SUMMARY\")\n",
    "    print(f\"\\nTotal documents loaded: {len(documents)}\")\n",
    "    print(f\"Unique terms in index: {len(all_terms)}\")\n",
    "\n",
    "    # Show most frequent terms\n",
    "    top_terms = sorted(inverted_index.items(), \n",
    "                      key=lambda x: len(x[1]), \n",
    "                      reverse=True)[:5]\n",
    "    print(\"\\nTop 5 terms:\")\n",
    "    for term, doc_ids in top_terms:\n",
    "        print(f\"  {term}: appears in {len(doc_ids)} documents\")\n",
    "\n",
    "    return inverted_index, document_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d3b27-f287-41a3-ac52-bd498b06aca8",
   "metadata": {},
   "source": [
    "### a) Levenshtein distance logic\n",
    "### b) Suggest terms for misspelled search strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8e21681-31bf-42cc-9c00-011139b0956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "    \n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    \n",
    "    prev_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        curr_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            inserts = prev_row[j + 1] + 1\n",
    "            deletes = curr_row[j] + 1\n",
    "            substitute = prev_row[j] + (c1 != c2)\n",
    "            curr_row.append(min(inserts, deletes, substitute))\n",
    "        prev_row = curr_row\n",
    "    \n",
    "    return prev_row[-1]\n",
    "\n",
    "def suggest_terms(misspelled_word, inverted_index, max_suggestions=5):\n",
    "    # First check for quick matches with common errors\n",
    "    suggestions = []\n",
    "    \n",
    "    # Calculate distances to all terms in our vocabulary\n",
    "    distances = []\n",
    "    for correct_word in inverted_index.keys():\n",
    "        distance = levenshtein(misspelled_word.lower(), correct_word.lower())\n",
    "        distances.append((correct_word, distance))\n",
    "    \n",
    "    # Sort by distance (closest first)\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Get the top N suggestions with smallest distance\n",
    "    closest_matches = [word for word, dist in distances[:max_suggestions]]\n",
    "    \n",
    "    return closest_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b09fe-7647-4c34-a487-3439484e9662",
   "metadata": {},
   "source": [
    "### a) Wildcard Search\n",
    "### b) Regular search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74051cdf-3965-48dc-8f57-cd4d6a452fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Functions\n",
    "def wildcard_search(query, inverted_index):\n",
    "    if not query.endswith('*'):\n",
    "        return []\n",
    "    prefix = query[:-1].lower()\n",
    "    return sorted([term for term in inverted_index.keys() \n",
    "                 if term.startswith(prefix)])\n",
    "\n",
    "def regular_search(query, inverted_index, doc_metadata):\n",
    "    terms = preprocess_text(query)\n",
    "    if not terms:\n",
    "        return []\n",
    "    \n",
    "    # Find documents containing ALL terms (AND logic)\n",
    "    matching_docs = None\n",
    "    for term in terms:\n",
    "        if term in inverted_index:\n",
    "            if matching_docs is None:\n",
    "                matching_docs = set(inverted_index[term])\n",
    "            else:\n",
    "                matching_docs.intersection_update(inverted_index[term])\n",
    "        else:\n",
    "            return []  # If any term doesn't exist, return nothing\n",
    "    \n",
    "    return list(matching_docs) if matching_docs else []\n",
    "\n",
    "def search(query, inverted_index, doc_metadata):\n",
    "    if query.endswith('*'):\n",
    "        terms = wildcard_search(query, inverted_index)\n",
    "        return {\n",
    "            'type': 'wildcard',\n",
    "            'query': query,\n",
    "            'count': len(terms),\n",
    "            'results': terms\n",
    "        }\n",
    "    else:\n",
    "        doc_ids = regular_search(query, inverted_index, doc_metadata)\n",
    "        results = []\n",
    "        for doc_id in doc_ids:\n",
    "            doc = doc_metadata[doc_id]\n",
    "            #preview = doc['content'][:100] + '...' if len(doc['content']) > 100 else doc['content']\n",
    "            results.append({\n",
    "                'doc_id': doc_id,\n",
    "                'filename': doc['filename'],\n",
    "                #'preview': preview\n",
    "            })\n",
    "        return {\n",
    "            'type': 'regular', \n",
    "            'query': query,\n",
    "            'count': len(results),\n",
    "            'results': results\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6852c29-e22f-4ffb-8e55-4dd83a3856e2",
   "metadata": {},
   "source": [
    "## Main Function to call all the user defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c1a45fe-a0cf-49d5-b2bf-3e74907f5865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the search system\"\"\"\n",
    "    print(\"Medical Document Search System\\n\")\n",
    "    \n",
    "    # Load documents\n",
    "    directory = \"D:/AIML/IR/Assignment/medical_documents/\"\n",
    "    print(directory)\n",
    "    \n",
    "    path = os.path.abspath(directory)\n",
    "    try:\n",
    "        inverted_index, document_metadata  =  load_documents(path) #load all the document from directory\n",
    "\n",
    "        print(\"\\n***************TESTING SEARCHES**********************\")\n",
    "         \n",
    "        while True:\n",
    "             print(\"\\nSearch options:\")\n",
    "             print(\"- Regular search: 'eg: diabetes'\")\n",
    "             print(\"- Wildcard search: 'eg: cardio*'\")\n",
    "             print(\"- Mispelled word: 'eg: cardeo'\")\n",
    "             print(\"Type 'exit' to quit\\n\")\n",
    "             query = input(\"\\nEnter Search term: \").strip()\n",
    "    \n",
    "             results = search(query, inverted_index, document_metadata)\n",
    "    \n",
    "             if results['type'] == 'wildcard':\n",
    "                print(f\"\\nFound {results['count']} matching terms:\")\n",
    "                for term in results['results'][:20]:  # Show first 20 matches\n",
    "                    print(f\"- {term} (in {len(inverted_index[term])} documents)\")\n",
    "                if results['count'] > 20:\n",
    "                    print(f\"... plus {results['count']-20} more terms\")\n",
    "             else:\n",
    "                if results['results']: \n",
    "                    print(f\"\\nFound {results['count']} documents:\")\n",
    "                    for doc in results['results']:\n",
    "                        print(f\"\\n{doc['filename']}\")\n",
    "                        #print(doc['preview'])\n",
    "                else:\n",
    "                    print(\"\\nNo direct matches found\")\n",
    "\n",
    "                print(\"\\nDid you mean: \")\n",
    "                suggestions = suggest_terms(query, inverted_index)\n",
    "                print(f\"'{query}': {suggestions}\")\n",
    "    \n",
    "    \n",
    "             if query.lower() == 'exit':\n",
    "                break\n",
    "\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda24a0b-dee0-472d-805d-cddfa8165274",
   "metadata": {},
   "source": [
    "## Main Function to call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27505218-9a99-48f5-bf08-eb79fc1c1a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Document Search System\n",
      "\n",
      "D:/AIML/IR/Assignment/medical_documents/\n",
      "Loading documents from: D:\\AIML\\IR\\Assignment\\medical_documents\n",
      "\n",
      "===============Loading: Cardio.pdf=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks\n",
      "Pranav Rajpurkar\u0003PRANAVSR @CS.STANFORD .EDU\n",
      "Awni Y. Hannun\u0003AWNI @CS.STANFORD .EDU\n",
      "Masoumeh Haghpanahi MHAGHPANAHI @IRHYTHMTEC...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "cardiologistlevel arrhythmia detection with convolutional neural networks\n",
      "pranav rajpurkarpranavsr csstanford edu\n",
      "awni y hannunawni csstanford edu\n",
      "masoumeh haghpanahi mhaghpanahi irhythmtech com\n",
      "codie...\n",
      "\n",
      "TOKENS (4517): ['cardiologistlevel', 'arrhythmia', 'detection', 'with', 'convolutional', 'neural', 'networks', 'pranav', 'rajpurkarpranavsr', 'csstanford', 'edu', 'awni', 'y', 'hannunawni', 'csstanford', 'edu', 'masoumeh', 'haghpanahi', 'mhaghpanahi', 'irhythmtech', 'com', 'codie', 'bourn', 'cbourn', 'irhythmtech', 'com', 'andrew', 'y', 'ng', 'ang'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (2853): ['cardiologistlevel', 'arrhythmia', 'detection', 'convolutional', 'neural', 'networks', 'pranav', 'rajpurkarpranavsr', 'csstanford', 'edu', 'awni', 'hannunawni', 'csstanford', 'edu', 'masoumeh', 'haghpanahi', 'mhaghpanahi', 'irhythmtech', 'com', 'codie', 'bourn', 'cbourn', 'irhythmtech', 'com', 'andrew', 'ang', 'csstanford', 'edu', 'abstract', 'develop'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (2853): ['cardiologistlevel', 'arrhythmia', 'detection', 'convolutional', 'neural', 'network', 'pranav', 'rajpurkarpranavsr', 'csstanford', 'edu', 'awni', 'hannunawni', 'csstanford', 'edu', 'masoumeh', 'haghpanahi', 'mhaghpanahi', 'irhythmtech', 'com', 'codie', 'bourn', 'cbourn', 'irhythmtech', 'com', 'andrew', 'ang', 'csstanford', 'edu', 'abstract', 'develop'] ...\n",
      "\n",
      "ðŸ“„ Cardio.pdf (.pdf)\n",
      "  - Terms: 2853\n",
      "  - Unique terms: 1264\n",
      "  - Sample terms: ['tool', '112', 'salvador', '64klters', '1062232237']...\n",
      "Loaded: Cardio.pdf\n",
      "\n",
      "===============Loading: Cardiovascular  Pulmonary.txt=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "Cardiovascular / Pulmonary\n",
      "\n",
      "Sample Name: Acute Inferior Myocardial Infarction\n",
      "\n",
      "Description: Patient presents with a chief complaint of chest pain admitted to Coronary Care Unit due to acute inferior m...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "cardiovascular  pulmonary\n",
      "\n",
      "sample name acute inferior myocardial infarction\n",
      "\n",
      "description patient presents with a chief complaint of chest pain admitted to coronary care unit due to acute inferior myoc...\n",
      "\n",
      "TOKENS (628): ['cardiovascular', 'pulmonary', 'sample', 'name', 'acute', 'inferior', 'myocardial', 'infarction', 'description', 'patient', 'presents', 'with', 'a', 'chief', 'complaint', 'of', 'chest', 'pain', 'admitted', 'to', 'coronary', 'care', 'unit', 'due', 'to', 'acute', 'inferior', 'myocardial', 'infarction', 'medical'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (397): ['cardiovascular', 'pulmonary', 'sample', 'name', 'acute', 'inferior', 'myocardial', 'infarction', 'description', 'patient', 'presents', 'chief', 'complaint', 'chest', 'pain', 'admitted', 'coronary', 'care', 'unit', 'due', 'acute', 'inferior', 'myocardial', 'infarction', 'medical', 'transcription', 'sample', 'report', 'chief', 'complaint'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (397): ['cardiovascular', 'pulmonary', 'sample', 'name', 'acute', 'inferior', 'myocardial', 'infarction', 'description', 'patient', 'present', 'chief', 'complaint', 'chest', 'pain', 'admitted', 'coronary', 'care', 'unit', 'due', 'acute', 'inferior', 'myocardial', 'infarction', 'medical', 'transcription', 'sample', 'report', 'chief', 'complaint'] ...\n",
      "\n",
      "ðŸ“„ Cardiovascular  Pulmonary.txt (.txt)\n",
      "  - Terms: 397\n",
      "  - Unique terms: 266\n",
      "  - Sample terms: ['unit', 'auscultation', '1150', '1040', 'motion']...\n",
      "Loaded: Cardiovascular  Pulmonary.txt\n",
      "\n",
      "===============Loading: DataAnalyticsinhealthcare.pdf=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/351792114\n",
      "Data Analytics in Healthcare Systems â€“ Principles, Challenges, and\n",
      "Appli...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "see discussions st ats and author pr ofiles f or this public ation at  httpswww researchgatene tpublic ation351792114\n",
      "data analytics in healthcare systems  principles challenges and\n",
      "applications\n",
      "chapt...\n",
      "\n",
      "TOKENS (8391): ['see', 'discussions', 'st', 'ats', 'and', 'author', 'pr', 'ofiles', 'f', 'or', 'this', 'public', 'ation', 'at', 'httpswww', 'researchgatene', 'tpublic', 'ation351792114', 'data', 'analytics', 'in', 'healthcare', 'systems', 'principles', 'challenges', 'and', 'applications', 'chapt', 'er', 'may'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (5366): ['see', 'discussions', 'ats', 'author', 'ofiles', 'public', 'ation', 'httpswww', 'researchgatene', 'tpublic', 'ation351792114', 'data', 'analytics', 'healthcare', 'systems', 'principles', 'challenges', 'applications', 'chapt', 'may', '2021', 'doi', '10120197810031852461', 'citations', '2reads', '10647', 'author', 'sug', 'anthi', 'galg'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (5366): ['see', 'discussion', 'at', 'author', 'ofiles', 'public', 'ation', 'httpswww', 'researchgatene', 'tpublic', 'ation351792114', 'data', 'analytics', 'healthcare', 'system', 'principle', 'challenge', 'application', 'chapt', 'may', '2021', 'doi', '10120197810031852461', 'citation', '2reads', '10647', 'author', 'sug', 'anthi', 'galg'] ...\n",
      "\n",
      "ðŸ“„ DataAnalyticsinhealthcare.pdf (.pdf)\n",
      "  - Terms: 5366\n",
      "  - Unique terms: 1843\n",
      "  - Sample terms: ['tool', 'earliest', '112', 'httpdx', 'accu']...\n",
      "Loaded: DataAnalyticsinhealthcare.pdf\n",
      "\n",
      "===============Loading: gender-differences-arteries.pdf=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "Adrien Desjardins2\n",
      "1R o y a lF r e eH o s p i t a l ,L o n d o n ,U n i t e dK i n g d o m ;2University College\n",
      "London, London, United Kingdom\n",
      "BACKGROUND In situ fenestration (ISF) is an attractive op...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "adrien desjardins2\n",
      "1r o y a lf r e eh o s p i t a l l o n d o n u n i t e dk i n g d o m 2university college\n",
      "london london united kingdom\n",
      "background in situ fenestration isf is an attractive option to...\n",
      "\n",
      "TOKENS (1133): ['adrien', 'desjardins2', '1r', 'o', 'y', 'a', 'lf', 'r', 'e', 'eh', 'o', 's', 'p', 'i', 't', 'a', 'l', 'l', 'o', 'n', 'd', 'o', 'n', 'u', 'n', 'i', 't', 'e', 'dk', 'i'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (666): ['adrien', 'desjardins2', '2university', 'college', 'london', 'london', 'united', 'kingdom', 'background', 'situ', 'fenestration', 'isf', 'attractive', 'option', 'preserve', 'aortic', 'branch', 'patency', 'fenestrated', 'endovascular', 'aorticrepair', 'fevar', 'complex', 'aortic', 'aneurysms', 'although', 'prefenestrated', 'grafts', 'suitable', 'common'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (666): ['adrien', 'desjardins2', '2university', 'college', 'london', 'london', 'united', 'kingdom', 'background', 'situ', 'fenestration', 'isf', 'attractive', 'option', 'preserve', 'aortic', 'branch', 'patency', 'fenestrated', 'endovascular', 'aorticrepair', 'fevar', 'complex', 'aortic', 'aneurysm', 'although', 'prefenestrated', 'graft', 'suitable', 'common'] ...\n",
      "\n",
      "ðŸ“„ gender-differences-arteries.pdf (.pdf)\n",
      "  - Terms: 666\n",
      "  - Unique terms: 467\n",
      "  - Sample terms: ['yin', 'abdominal', 'balloon', 'research', 'infrapoplitea']...\n",
      "Loaded: gender-differences-arteries.pdf\n",
      "\n",
      "===============Loading: in-hospital-mortality-trends-by-health-category.csv=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "05/2018 Anxiety Ambulatory Surgery 09/2018 Anxiety Ambulatory Surgery 10/2018 Anxiety Ambulatory Surgery 01/2019 Anxiety Ambulatory Surgery 06/2019 Anxiety Ambulatory Surgery 02/2020 Anxiety Ambulator...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "052018 anxiety ambulatory surgery 092018 anxiety ambulatory surgery 102018 anxiety ambulatory surgery 012019 anxiety ambulatory surgery 062019 anxiety ambulatory surgery 022020 anxiety ambulatory surg...\n",
      "\n",
      "TOKENS (10907): ['052018', 'anxiety', 'ambulatory', 'surgery', '092018', 'anxiety', 'ambulatory', 'surgery', '102018', 'anxiety', 'ambulatory', 'surgery', '012019', 'anxiety', 'ambulatory', 'surgery', '062019', 'anxiety', 'ambulatory', 'surgery', '022020', 'anxiety', 'ambulatory', 'surgery', '032020', 'anxiety', 'ambulatory', 'surgery', '042020', 'anxiety'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (10907): ['052018', 'anxiety', 'ambulatory', 'surgery', '092018', 'anxiety', 'ambulatory', 'surgery', '102018', 'anxiety', 'ambulatory', 'surgery', '012019', 'anxiety', 'ambulatory', 'surgery', '062019', 'anxiety', 'ambulatory', 'surgery', '022020', 'anxiety', 'ambulatory', 'surgery', '032020', 'anxiety', 'ambulatory', 'surgery', '042020', 'anxiety'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (10907): ['052018', 'anxiety', 'ambulatory', 'surgery', '092018', 'anxiety', 'ambulatory', 'surgery', '102018', 'anxiety', 'ambulatory', 'surgery', '012019', 'anxiety', 'ambulatory', 'surgery', '062019', 'anxiety', 'ambulatory', 'surgery', '022020', 'anxiety', 'ambulatory', 'surgery', '032020', 'anxiety', 'ambulatory', 'surgery', '042020', 'anxiety'] ...\n",
      "\n",
      "ðŸ“„ in-hospital-mortality-trends-by-health-category.csv (.csv)\n",
      "  - Terms: 10907\n",
      "  - Unique terms: 123\n",
      "  - Sample terms: ['52021', '42018', '062019', '102018', '052021']...\n",
      "Loaded: in-hospital-mortality-trends-by-health-category.csv\n",
      "\n",
      "===============Loading: Medical Specialty.txt=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "Medical Specialty:\n",
      "Cardiovascular / Pulmonary\n",
      "\n",
      "Sample Name: Abnormal Echocardiogram\n",
      "\n",
      "Description: Abnormal echocardiogram findings and followup. Shortness of breath, congestive heart failure, and valv...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "medical specialty\n",
      "cardiovascular  pulmonary\n",
      "\n",
      "sample name abnormal echocardiogram\n",
      "\n",
      "description abnormal echocardiogram findings and followup shortness of breath congestive heart failure and valvular in...\n",
      "\n",
      "TOKENS (567): ['medical', 'specialty', 'cardiovascular', 'pulmonary', 'sample', 'name', 'abnormal', 'echocardiogram', 'description', 'abnormal', 'echocardiogram', 'findings', 'and', 'followup', 'shortness', 'of', 'breath', 'congestive', 'heart', 'failure', 'and', 'valvular', 'insufficiency', 'the', 'patient', 'complains', 'of', 'shortness', 'of', 'breath'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (379): ['medical', 'specialty', 'cardiovascular', 'pulmonary', 'sample', 'name', 'abnormal', 'echocardiogram', 'description', 'abnormal', 'echocardiogram', 'findings', 'followup', 'shortness', 'breath', 'congestive', 'heart', 'failure', 'valvular', 'insufficiency', 'patient', 'complains', 'shortness', 'breath', 'worsening', 'patient', 'underwent', 'echocardiogram', 'shows', 'severe'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (379): ['medical', 'specialty', 'cardiovascular', 'pulmonary', 'sample', 'name', 'abnormal', 'echocardiogram', 'description', 'abnormal', 'echocardiogram', 'finding', 'followup', 'shortness', 'breath', 'congestive', 'heart', 'failure', 'valvular', 'insufficiency', 'patient', 'complains', 'shortness', 'breath', 'worsening', 'patient', 'underwent', 'echocardiogram', 'show', 'severe'] ...\n",
      "\n",
      "ðŸ“„ Medical Specialty.txt (.txt)\n",
      "  - Terms: 379\n",
      "  - Unique terms: 237\n",
      "  - Sample terms: ['many', 'underwent', 'distally', 'frequency', 'presently']...\n",
      "Loaded: Medical Specialty.txt\n",
      "\n",
      "===============Loading: Medical Specialty_Gastro.pdf=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "Medical Specialty:  \n",
      "Gastroenterology  \n",
      " \n",
      "Sample Name:  Colonoscopy & Polypectomy - 3 \n",
      " \n",
      "Description:  Total colonoscopy with biopsy and snare polypectomy.  \n",
      "(Medical Transcription Sample Report)  \n",
      "PR...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "medical specialty  \n",
      "gastroenterology  \n",
      " \n",
      "sample name  colonoscopy  polypectomy  3 \n",
      " \n",
      "description  total colonoscopy with biopsy and snare polypectomy  \n",
      "medical transcription sample report  \n",
      "preoperati...\n",
      "\n",
      "TOKENS (336): ['medical', 'specialty', 'gastroenterology', 'sample', 'name', 'colonoscopy', 'polypectomy', '3', 'description', 'total', 'colonoscopy', 'with', 'biopsy', 'and', 'snare', 'polypectomy', 'medical', 'transcription', 'sample', 'report', 'preoperative', 'diagnosis', 'alternating', 'hard', 'and', 'soft', 'stools', 'postoperative', 'diagnosis', 'sigmoid'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (209): ['medical', 'specialty', 'gastroenterology', 'sample', 'name', 'colonoscopy', 'polypectomy', 'description', 'total', 'colonoscopy', 'biopsy', 'snare', 'polypectomy', 'medical', 'transcription', 'sample', 'report', 'preoperative', 'diagnosis', 'alternating', 'hard', 'soft', 'stools', 'postoperative', 'diagnosis', 'sigmoid', 'diverticulosis', 'sessile', 'polyp', 'sigmoid'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (209): ['medical', 'specialty', 'gastroenterology', 'sample', 'name', 'colonoscopy', 'polypectomy', 'description', 'total', 'colonoscopy', 'biopsy', 'snare', 'polypectomy', 'medical', 'transcription', 'sample', 'report', 'preoperative', 'diagnosis', 'alternating', 'hard', 'soft', 'stool', 'postoperative', 'diagnosis', 'sigmoid', 'diverticulosis', 'sessile', 'polyp', 'sigmoid'] ...\n",
      "\n",
      "ðŸ“„ Medical Specialty_Gastro.pdf (.pdf)\n",
      "  - Terms: 209\n",
      "  - Unique terms: 132\n",
      "  - Sample terms: ['adenomatous', 'alternating', 'old', 'one', 'sent']...\n",
      "Loaded: Medical Specialty_Gastro.pdf\n",
      "\n",
      "===============Loading: Medical_history.docx=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "Medical Specialty:\n",
      "Surgery\n",
      "\n",
      "Sample Name:Â Arthroscopy & Chondroplasty\n",
      "\n",
      "Description:Â Diagnostic arthroscopy with partial chondroplasty of patella, lateral retinacular release, and open tibial tubercle t...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "medical specialty\n",
      "surgery\n",
      "\n",
      "sample nameÂ arthroscopy  chondroplasty\n",
      "\n",
      "descriptionÂ diagnostic arthroscopy with partial chondroplasty of patella lateral retinacular release and open tibial tubercle transfe...\n",
      "\n",
      "TOKENS (716): ['medical', 'specialty', 'surgery', 'sample', 'name', 'arthroscopy', 'chondroplasty', 'description', 'diagnostic', 'arthroscopy', 'with', 'partial', 'chondroplasty', 'of', 'patella', 'lateral', 'retinacular', 'release', 'and', 'open', 'tibial', 'tubercle', 'transfer', 'with', 'fixation', 'of', 'two', '45', 'mm', 'cannulated'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (417): ['medical', 'specialty', 'surgery', 'sample', 'name', 'arthroscopy', 'chondroplasty', 'description', 'diagnostic', 'arthroscopy', 'partial', 'chondroplasty', 'patella', 'lateral', 'retinacular', 'release', 'open', 'tibial', 'tubercle', 'transfer', 'fixation', 'two', 'cannulated', 'screws', 'gradeiv', 'chondromalacia', 'patella', 'patellofemoral', 'malalignment', 'syndrome'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (417): ['medical', 'specialty', 'surgery', 'sample', 'name', 'arthroscopy', 'chondroplasty', 'description', 'diagnostic', 'arthroscopy', 'partial', 'chondroplasty', 'patella', 'lateral', 'retinacular', 'release', 'open', 'tibial', 'tubercle', 'transfer', 'fixation', 'two', 'cannulated', 'screw', 'gradeiv', 'chondromalacia', 'patella', 'patellofemoral', 'malalignment', 'syndrome'] ...\n",
      "\n",
      "ðŸ“„ Medical_history.docx (.docx)\n",
      "  - Terms: 417\n",
      "  - Unique terms: 249\n",
      "  - Sample terms: ['failed', 'transfer', 'central', 'cut', 'caucasian']...\n",
      "Loaded: Medical_history.docx\n",
      "\n",
      "===============Loading: mtsamples.csv=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      " A 23-year-old white female presents with complaint of allergies.  Allergy / Immunology  Allergic Rhinitis  SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used ...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      " a 23yearold white female presents with complaint of allergies  allergy  immunology  allergic rhinitis  subjective  this 23yearold white female presents with complaint of allergies  she used to have a...\n",
      "\n",
      "TOKENS (68702): ['a', '23yearold', 'white', 'female', 'presents', 'with', 'complaint', 'of', 'allergies', 'allergy', 'immunology', 'allergic', 'rhinitis', 'subjective', 'this', '23yearold', 'white', 'female', 'presents', 'with', 'complaint', 'of', 'allergies', 'she', 'used', 'to', 'have', 'allergies', 'when', 'she'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (49840): ['23yearold', 'white', 'female', 'presents', 'complaint', 'allergies', 'allergy', 'immunology', 'allergic', 'rhinitis', 'subjective', '23yearold', 'white', 'female', 'presents', 'complaint', 'allergies', 'used', 'allergies', 'lived', 'seattle', 'thinks', 'worse', 'past', 'tried', 'claritin', 'zyrtec', 'worked', 'short', 'time'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (49840): ['23yearold', 'white', 'female', 'present', 'complaint', 'allergy', 'allergy', 'immunology', 'allergic', 'rhinitis', 'subjective', '23yearold', 'white', 'female', 'present', 'complaint', 'allergy', 'used', 'allergy', 'lived', 'seattle', 'think', 'worse', 'past', 'tried', 'claritin', 'zyrtec', 'worked', 'short', 'time'] ...\n",
      "\n",
      "ðŸ“„ mtsamples.csv (.csv)\n",
      "  - Terms: 49840\n",
      "  - Unique terms: 4646\n",
      "  - Sample terms: ['sufficient', 'ileocecal', 'hemoclipped', 'obvious', '112']...\n",
      "Loaded: mtsamples.csv\n",
      "\n",
      "===============Loading: mtsamples.xlsx=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      " A 23-year-old white female presents with complaint of allergies.  Allergy / Immunology  Allergic Rhinitis  SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used ...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      " a 23yearold white female presents with complaint of allergies  allergy  immunology  allergic rhinitis  subjective  this 23yearold white female presents with complaint of allergies  she used to have a...\n",
      "\n",
      "TOKENS (44892): ['a', '23yearold', 'white', 'female', 'presents', 'with', 'complaint', 'of', 'allergies', 'allergy', 'immunology', 'allergic', 'rhinitis', 'subjective', 'this', '23yearold', 'white', 'female', 'presents', 'with', 'complaint', 'of', 'allergies', 'she', 'used', 'to', 'have', 'allergies', 'when', 'she'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (26030): ['23yearold', 'white', 'female', 'presents', 'complaint', 'allergies', 'allergy', 'immunology', 'allergic', 'rhinitis', 'subjective', '23yearold', 'white', 'female', 'presents', 'complaint', 'allergies', 'used', 'allergies', 'lived', 'seattle', 'thinks', 'worse', 'past', 'tried', 'claritin', 'zyrtec', 'worked', 'short', 'time'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (26030): ['23yearold', 'white', 'female', 'present', 'complaint', 'allergy', 'allergy', 'immunology', 'allergic', 'rhinitis', 'subjective', '23yearold', 'white', 'female', 'present', 'complaint', 'allergy', 'used', 'allergy', 'lived', 'seattle', 'think', 'worse', 'past', 'tried', 'claritin', 'zyrtec', 'worked', 'short', 'time'] ...\n",
      "\n",
      "ðŸ“„ mtsamples.xlsx (.xlsx)\n",
      "  - Terms: 26030\n",
      "  - Unique terms: 4647\n",
      "  - Sample terms: ['sufficient', 'ileocecal', 'hemoclipped', 'obvious', '112']...\n",
      "Loaded: mtsamples.xlsx\n",
      "\n",
      "===============Loading: train_1.txt=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "4\tCatheterization laboratory events and hospital outcome with direct angioplasty for acute myocardial infarction To assess the safety of direct infarct angioplasty without antecedent thrombolytic ther...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "4\tcatheterization laboratory events and hospital outcome with direct angioplasty for acute myocardial infarction to assess the safety of direct infarct angioplasty without antecedent thrombolytic ther...\n",
      "\n",
      "TOKENS (2157): ['4', 'catheterization', 'laboratory', 'events', 'and', 'hospital', 'outcome', 'with', 'direct', 'angioplasty', 'for', 'acute', 'myocardial', 'infarction', 'to', 'assess', 'the', 'safety', 'of', 'direct', 'infarct', 'angioplasty', 'without', 'antecedent', 'thrombolytic', 'therapy', 'catheterization', 'laboratory', 'and', 'hospital'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (1255): ['catheterization', 'laboratory', 'events', 'hospital', 'outcome', 'direct', 'angioplasty', 'acute', 'myocardial', 'infarction', 'assess', 'safety', 'direct', 'infarct', 'angioplasty', 'without', 'antecedent', 'thrombolytic', 'therapy', 'catheterization', 'laboratory', 'hospital', 'events', 'assessed', 'consecutively', 'treated', 'patients', 'infarctions', 'involving', 'left'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (1255): ['catheterization', 'laboratory', 'event', 'hospital', 'outcome', 'direct', 'angioplasty', 'acute', 'myocardial', 'infarction', 'assess', 'safety', 'direct', 'infarct', 'angioplasty', 'without', 'antecedent', 'thrombolytic', 'therapy', 'catheterization', 'laboratory', 'hospital', 'event', 'assessed', 'consecutively', 'treated', 'patient', 'infarction', 'involving', 'left'] ...\n",
      "\n",
      "ðŸ“„ train_1.txt (.txt)\n",
      "  - Terms: 1255\n",
      "  - Unique terms: 629\n",
      "  - Sample terms: ['misdirected', 'modest', 'one', 'rat', 'appendicitis']...\n",
      "Loaded: train_1.txt\n",
      "\n",
      "===============Loading: Train_Data.txt=====================================\n",
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "###24293578\n",
      "OBJECTIVE\tTo investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the ef...\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "24293578\n",
      "objective\tto investigate the efficacy of 6 weeks of daily lowdose oral prednisolone in improving pain  mobility  and systemic lowgrade inflammation in the short term and whether the effect wo...\n",
      "\n",
      "TOKENS (5539): ['24293578', 'objective', 'to', 'investigate', 'the', 'efficacy', 'of', '6', 'weeks', 'of', 'daily', 'lowdose', 'oral', 'prednisolone', 'in', 'improving', 'pain', 'mobility', 'and', 'systemic', 'lowgrade', 'inflammation', 'in', 'the', 'short', 'term', 'and', 'whether', 'the', 'effect'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (3453): ['24293578', 'objective', 'investigate', 'efficacy', 'weeks', 'daily', 'lowdose', 'oral', 'prednisolone', 'improving', 'pain', 'mobility', 'systemic', 'lowgrade', 'inflammation', 'short', 'term', 'whether', 'effect', 'would', 'sustained', 'weeks', 'older', 'adults', 'moderate', 'severe', 'knee', 'osteoarthritis', 'methods', 'total'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (3453): ['24293578', 'objective', 'investigate', 'efficacy', 'week', 'daily', 'lowdose', 'oral', 'prednisolone', 'improving', 'pain', 'mobility', 'systemic', 'lowgrade', 'inflammation', 'short', 'term', 'whether', 'effect', 'would', 'sustained', 'week', 'older', 'adult', 'moderate', 'severe', 'knee', 'osteoarthritis', 'method', 'total'] ...\n",
      "\n",
      "ðŸ“„ Train_Data.txt (.txt)\n",
      "  - Terms: 3453\n",
      "  - Unique terms: 1344\n",
      "  - Sample terms: ['registered', '0024', 'ankle', 'balloon', '053']...\n",
      "Loaded: Train_Data.txt\n",
      "\n",
      "TOTAL SUMMARY\n",
      "\n",
      "Total documents loaded: 12\n",
      "Unique terms in index: 8116\n",
      "\n",
      "Top 5 terms:\n",
      "  patient: appears in 11 documents\n",
      "  disease: appears in 11 documents\n",
      "  risk: appears in 10 documents\n",
      "  heart: appears in 9 documents\n",
      "  also: appears in 9 documents\n",
      "\n",
      "***************TESTING SEARCHES**********************\n",
      "\n",
      "Search options:\n",
      "- Regular search: 'eg: diabetes'\n",
      "- Wildcard search: 'eg: cardio*'\n",
      "- Mispelled word: 'eg: cardeo'\n",
      "Type 'exit' to quit\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Search term:  cardiology\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "cardiology\n",
      "=== AFTER CLEANING ===\n",
      "cardiology\n",
      "TOKENS (1): ['cardiology'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (1): ['cardiology'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (1): ['cardiology'] ...\n",
      "\n",
      "Found 4 documents:\n",
      "\n",
      "Cardio.pdf\n",
      "\n",
      "mtsamples.csv\n",
      "\n",
      "gender-differences-arteries.pdf\n",
      "\n",
      "mtsamples.xlsx\n",
      "\n",
      "Did you mean: \n",
      "'cardiology': ['cardiology', 'cardiolo', 'radiology', 'cardiologist', 'cardiol']\n",
      "\n",
      "Search options:\n",
      "- Regular search: 'eg: diabetes'\n",
      "- Wildcard search: 'eg: cardio*'\n",
      "- Mispelled word: 'eg: cardeo'\n",
      "Type 'exit' to quit\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Search term:  cardio*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 12 matching terms:\n",
      "- cardio (in 2 documents)\n",
      "- cardiogenic (in 1 documents)\n",
      "- cardiographic (in 1 documents)\n",
      "- cardiol (in 1 documents)\n",
      "- cardiolo (in 1 documents)\n",
      "- cardiologist (in 2 documents)\n",
      "- cardiologistlevel (in 1 documents)\n",
      "- cardiology (in 4 documents)\n",
      "- cardiopulmonary (in 4 documents)\n",
      "- cardiovascular (in 9 documents)\n",
      "- cardioversion (in 1 documents)\n",
      "- cardioverter (in 1 documents)\n",
      "\n",
      "Search options:\n",
      "- Regular search: 'eg: diabetes'\n",
      "- Wildcard search: 'eg: cardio*'\n",
      "- Mispelled word: 'eg: cardeo'\n",
      "Type 'exit' to quit\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Search term:  heart\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "heart\n",
      "=== AFTER CLEANING ===\n",
      "heart\n",
      "TOKENS (1): ['heart'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (1): ['heart'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (1): ['heart'] ...\n",
      "\n",
      "Found 9 documents:\n",
      "\n",
      "Cardio.pdf\n",
      "\n",
      "Cardiovascular  Pulmonary.txt\n",
      "\n",
      "DataAnalyticsinhealthcare.pdf\n",
      "\n",
      "gender-differences-arteries.pdf\n",
      "\n",
      "Medical Specialty.txt\n",
      "\n",
      "mtsamples.csv\n",
      "\n",
      "mtsamples.xlsx\n",
      "\n",
      "train_1.txt\n",
      "\n",
      "Train_Data.txt\n",
      "\n",
      "Did you mean: \n",
      "'heart': ['heart', 'healt', 'pert', 'wear', 'ear']\n",
      "\n",
      "Search options:\n",
      "- Regular search: 'eg: diabetes'\n",
      "- Wildcard search: 'eg: cardio*'\n",
      "- Mispelled word: 'eg: cardeo'\n",
      "Type 'exit' to quit\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Search term:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ORIGINAL TEXT (SAMPLE) ===\n",
      "exit\n",
      "=== AFTER CLEANING ===\n",
      "exit\n",
      "TOKENS (1): ['exit'] ...\n",
      "\n",
      "AFTER STOPWORD REMOVAL (1): ['exit'] ...\n",
      "\n",
      "FINAL PROCESSED TERMS (1): ['exit'] ...\n",
      "\n",
      "Found 1 documents:\n",
      "\n",
      "train_1.txt\n",
      "\n",
      "Did you mean: \n",
      "'exit': ['exit', 'exist', 'exam', 'exact', 'eric']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
